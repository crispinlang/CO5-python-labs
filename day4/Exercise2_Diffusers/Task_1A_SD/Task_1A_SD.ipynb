{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56f4cf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "torch\n",
    "matplotlib\n",
    "scikit-learn\n",
    "diffusers\n",
    "transformers\n",
    "scipy\n",
    "ftfy\n",
    "accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8d3acb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:fg:1: no job control in this shell.\n"
     ]
    }
   ],
   "source": [
    "!%pip install -r requirements.txt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler, DDIMScheduler\n",
    "from tqdm.auto import tqdm\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina' # make plots prettier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2535ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 24.07it/s]\n",
      " 37%|███▋      | 37/100 [01:40<01:46,  1.69s/it]"
     ]
    }
   ],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "repo_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "device = \"cpu\"\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(repo_id, subfolder=\"vae\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "unet = UNet2DConditionModel.from_pretrained(repo_id, subfolder=\"unet\")\n",
    "\n",
    "vae = vae.to(device)\n",
    "text_encoder = text_encoder.to(device)\n",
    "unet = unet.to(device)\n",
    "\n",
    "scheduler = DDIMScheduler.from_pretrained(repo_id, subfolder=\"scheduler\")\n",
    "\n",
    "num_inference_steps = 100\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "def get_text_embeddings(prompt):\n",
    "    text_ids = tokenizer(\n",
    "        prompt, \n",
    "        padding=\"max_length\", \n",
    "        max_length=tokenizer.model_max_length, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(text_ids)[0]\n",
    "    \n",
    "    return text_embeddings\n",
    "\n",
    "###\n",
    "prompt = \"House of a Swiss Family in the Swiss Alps, scenic view, beautiful lighting, ultra detailed, 8k\"\n",
    "text_embeddings = get_text_embeddings(prompt)\n",
    "text_embeddings.shape\n",
    "\n",
    "height = 512\n",
    "width = 512\n",
    "batch_size = 1\n",
    "in_channels = unet.config.in_channels\n",
    "\n",
    "def get_latents():\n",
    "    latents = torch.randn(\n",
    "        (batch_size, in_channels, height // 8, width // 8),\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    return latents\n",
    "\n",
    "latents = get_latents()\n",
    "\n",
    "###\n",
    "for t in tqdm(scheduler.timesteps):\n",
    "    latent_model_input = scheduler.scale_model_input(latents, t)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "###\n",
    "latents = latents / vae.config.scaling_factor\n",
    "\n",
    "with torch.no_grad():\n",
    "    image = vae.decode(latents).sample\n",
    "\n",
    "image.min(), image.max()\n",
    "\n",
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "image.shape\n",
    "\n",
    "plt.imshow(image[0].permute(1, 2, 0).detach().cpu())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
