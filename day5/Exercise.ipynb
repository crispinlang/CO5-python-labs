{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udcda  Exercise Histopathology Foundation Model\n",
        "\n",
        "Exercise adapted from Charlotte Bunne, EPFL\n",
        "\n",
        "This exercise needs to be run on a Modal notebook. \n",
        "\n",
        "\n",
        "- [Subtask 1.0: Get familiar with Histopathology images](#Subtask-1.0:-Get-familiar-with-Histopathology-images)\n",
        "- [Subtask 1.1: Linear Probing based on FM embeddings](#task-11-linear-probing-based-on-fm-embeddings)\n",
        "- [Subtask 1.2: Tissue-level classification](#task-12-tissue-level-classification)\n",
        "- [Subtask 1.3: Whole-slide image analysis](#task-13-whole-slide-image-analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Histopathology Foundation Models\n",
        "\n",
        "In this task we are taking a look at one prominent area of Vision Foundation Models trained using DINOv2: Histopathology.\n",
        "Since there is a bunch of different clincal \"downstream\" tasks on which one can possibly utilize large FMs, there is a variety of benchmarks available to, e.g., predict tumor (sub-)types, gene expressions within areas of the tissue, interesting Regions of Interest (ROIs). An overview of a few different and commonly used pathology models can be found here: https://birkhoffkiki.github.io/PathBench/. For this exercise, we are using the Phikon2 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "!pip install torch transformers datasets Pillow umap-learn\n",
        "!pip install s3fs tiffile imagecodecs zarr scikit-image\n",
        "!pip install matplotlib seaborn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/site-packages (2.8.0+cu129)\r\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/site-packages (4.56.0)\r\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/site-packages (4.5.0)\r\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/site-packages (11.0.0)\r\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.12/site-packages (0.5.11)\r\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch) (3.13.1)\r\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/site-packages (from torch) (4.12.2)\r\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch) (70.2.0)\r\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/site-packages (from torch) (1.13.3)\r\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from torch) (3.3)\r\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch) (3.1.4)\r\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from torch) (2026.1.0)\r\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.9.86 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.86)\r\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.79)\r\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.79)\r\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/site-packages (from torch) (9.10.2.21)\r\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.9.1.4 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.1.4)\r\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.4.1.4 in /usr/local/lib/python3.12/site-packages (from torch) (11.4.1.4)\r\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.10.19 in /usr/local/lib/python3.12/site-packages (from torch) (10.3.10.19)\r\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.5.82 in /usr/local/lib/python3.12/site-packages (from torch) (11.7.5.82)\r\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.10.65 in /usr/local/lib/python3.12/site-packages (from torch) (12.5.10.65)\r\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/site-packages (from torch) (0.7.1)\r\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/site-packages (from torch) (2.27.3)\r\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.79)\r\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.9.86 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.86)\r\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.14.1.1 in /usr/local/lib/python3.12/site-packages (from torch) (1.14.1.1)\r\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/site-packages (from torch) (3.4.0)\r\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/site-packages (from transformers) (0.34.4)\r\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/site-packages (from transformers) (2.1.2)\r\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/site-packages (from transformers) (25.0)\r\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/site-packages (from transformers) (6.0.2)\r\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/site-packages (from transformers) (2025.9.1)\r\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from transformers) (2.32.5)\r\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/site-packages (from transformers) (0.22.0)\r\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/site-packages (from transformers) (0.6.2)\r\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/site-packages (from transformers) (4.67.1)\r\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/site-packages (from datasets) (23.0.0)\r\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/site-packages (from datasets) (0.4.0)\r\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/site-packages (from datasets) (2.3.2)\r\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/site-packages (from datasets) (0.28.1)\r\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/site-packages (from datasets) (3.6.0)\r\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/site-packages (from datasets) (0.70.18)\r\n",
            "Collecting fsspec (from torch)\r\n",
            "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\r\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.12/site-packages (from umap-learn) (1.16.1)\r\n",
            "Requirement already satisfied: scikit-learn>=1.6 in /usr/local/lib/python3.12/site-packages (from umap-learn) (1.7.1)\r\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.12/site-packages (from umap-learn) (0.61.2)\r\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.12/site-packages (from umap-learn) (0.6.0)\r\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\r\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.10.0)\r\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2024.8.30)\r\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\r\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.10)\r\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\r\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\r\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.12/site-packages (from numba>=0.51.2->umap-learn) (0.44.0)\r\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.12/site-packages (from pynndescent>=0.5->umap-learn) (1.5.2)\r\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\r\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn>=1.6->umap-learn) (3.6.0)\r\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\r\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\r\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\r\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\r\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\r\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\r\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.1)\r\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.1.0)\r\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\r\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\r\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\r\n",
            "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\r\n",
            "Installing collected packages: fsspec\r\n",
            "  Attempting uninstall: fsspec\r\n",
            "    Found existing installation: fsspec 2026.1.0\r\n",
            "    Uninstalling fsspec-2026.1.0:\r\n",
            "      Successfully uninstalled fsspec-2026.1.0\r\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
            "s3fs 2026.1.0 requires fsspec==2026.1.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
            "\u001b[0mSuccessfully installed fsspec-2025.10.0\r\n",
            "\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
            "Requirement already satisfied: s3fs in /usr/local/lib/python3.12/site-packages (2026.1.0)\r\n",
            "Requirement already satisfied: tiffile in /usr/local/lib/python3.12/site-packages (2018.10.18)\r\n",
            "Requirement already satisfied: imagecodecs in /usr/local/lib/python3.12/site-packages (2026.1.14)\r\n",
            "Requirement already satisfied: zarr in /usr/local/lib/python3.12/site-packages (3.1.5)\r\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/site-packages (0.25.2)\r\n",
            "Requirement already satisfied: aiobotocore<4.0.0,>=2.5.4 in /usr/local/lib/python3.12/site-packages (from s3fs) (3.1.1)\r\n",
            "Collecting fsspec==2026.1.0 (from s3fs)\r\n",
            "  Downloading fsspec-2026.1.0-py3-none-any.whl.metadata (10 kB)\r\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/site-packages (from s3fs) (3.13.3)\r\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.12/site-packages (from tiffile) (2025.8.28)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (from imagecodecs) (2.1.2)\r\n",
            "Requirement already satisfied: donfig>=0.8 in /usr/local/lib/python3.12/site-packages (from zarr) (0.8.1.post1)\r\n",
            "Requirement already satisfied: google-crc32c>=1.5 in /usr/local/lib/python3.12/site-packages (from zarr) (1.8.0)\r\n",
            "Requirement already satisfied: numcodecs>=0.14 in /usr/local/lib/python3.12/site-packages (from zarr) (0.16.5)\r\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.12/site-packages (from zarr) (25.0)\r\n",
            "Requirement already satisfied: typing-extensions>=4.9 in /usr/local/lib/python3.12/site-packages (from zarr) (4.12.2)\r\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/site-packages (from scikit-image) (1.16.1)\r\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/site-packages (from scikit-image) (3.3)\r\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/site-packages (from scikit-image) (11.0.0)\r\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/site-packages (from scikit-image) (2.37.0)\r\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/site-packages (from scikit-image) (0.4)\r\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/site-packages (from aiobotocore<4.0.0,>=2.5.4->s3fs) (0.13.0)\r\n",
            "Requirement already satisfied: botocore<1.42.31,>=1.41.0 in /usr/local/lib/python3.12/site-packages (from aiobotocore<4.0.0,>=2.5.4->s3fs) (1.42.30)\r\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/site-packages (from aiobotocore<4.0.0,>=2.5.4->s3fs) (2.9.0.post0)\r\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/site-packages (from aiobotocore<4.0.0,>=2.5.4->s3fs) (1.0.1)\r\n",
            "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.12/site-packages (from aiobotocore<4.0.0,>=2.5.4->s3fs) (6.1.0)\r\n",
            "Requirement already satisfied: wrapt<3.0.0,>=1.10.10 in /usr/local/lib/python3.12/site-packages (from aiobotocore<4.0.0,>=2.5.4->s3fs) (1.17.3)\r\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.6.1)\r\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.4.0)\r\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (24.2.0)\r\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.4.1)\r\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.4.1)\r\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.22.0)\r\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/site-packages (from donfig>=0.8->zarr) (6.0.2)\r\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/site-packages (from botocore<1.42.31,>=1.41.0->aiobotocore<4.0.0,>=2.5.4->s3fs) (2.5.0)\r\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore<4.0.0,>=2.5.4->s3fs) (1.17.0)\r\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.10)\r\n",
            "Downloading fsspec-2026.1.0-py3-none-any.whl (201 kB)\r\n",
            "Installing collected packages: fsspec\r\n",
            "  Attempting uninstall: fsspec\r\n",
            "    Found existing installation: fsspec 2025.10.0\r\n",
            "    Uninstalling fsspec-2025.10.0:\r\n",
            "      Successfully uninstalled fsspec-2025.10.0\r\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
            "datasets 4.5.0 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
            "\u001b[0mSuccessfully installed fsspec-2026.1.0\r\n",
            "\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/site-packages (3.10.6)\r\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/site-packages (0.13.2)\r\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/site-packages (from matplotlib) (1.3.3)\r\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\r\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/site-packages (from matplotlib) (4.59.2)\r\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib) (1.4.9)\r\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/site-packages (from matplotlib) (2.1.2)\r\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/site-packages (from matplotlib) (25.0)\r\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/site-packages (from matplotlib) (11.0.0)\r\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib) (3.2.3)\r\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\r\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/site-packages (from seaborn) (2.3.2)\r\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.2)\r\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.2)\r\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n",
            "\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "try:\n",
        "    from umap.umap_ import UMAP\n",
        "except ImportError:\n",
        "    from umap import UMAP"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"task1.0\"></a>\n",
        "\n",
        "### Task 1.0: Getting familiar with Histopathology images.\n",
        "\n",
        "**Histopathology background:** <br>\n",
        "Histopathology is the microscopic examination of tissue samples to study the manifestations of disease. It is widely used in medical diagnosis to identify abnormalities such as cancer, inflammation, and infections. <br>\n",
        "One of the fundamental staining techniques in histopathology is Hematoxylin and Eosin (H&E) staining. This method uses two dyes: hematoxylin and eosin. Hematoxylin stains cell nuclei a purplish-blue color, highlighting DNA and nuclear structures, while eosin stains the cytoplasm and extracellular matrix various shades of pink or red. The color contrast provided by H&E staining allows pathologists to easily differentiate cellular components and assess tissue architecture and pathological changes effectively. <br>\n",
        "H&E staining is valued for its simplicity, cost-effectiveness, and ability to provide detailed visualization of tissue structure, making it the gold standard in histopathology laboratories globally. <br>\n",
        "\n",
        "In the following, we are taking a look at H&E stains of lung tissue samples. More precisly, we are trying to label the lung cancer type of the H&E stains to distinguish between Lung adenocarcinoma (LUAD) and Lung squamous cell carcinoma (LUSC). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# if you get an error here, try setting streaming=False. \n",
        "# This will download the dataset locally in advance.\n",
        "ds_train = load_dataset(\"dakomura/tcga-ut\", \"internal\", split=\"train\", streaming=True) \n",
        "\n",
        "ds_train = ds_train.filter(lambda sample: \"Lung\" in sample[\"json\"][\"label\"])\n",
        "\n",
        "ds_test = load_dataset(\"dakomura/tcga-ut\", \"internal\", split=\"test\", streaming=True)\n",
        "ds_test = ds_test.filter(lambda sample: \"Lung\" in sample[\"json\"][\"label\"])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1627630bd894fd5b790e056e6db717c",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Resolving data files:   0%|          | 0/39 [00:00<?, ?it/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9c9b4c4c5864546a9dc87274eaddc77",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Resolving data files:   0%|          | 0/39 [00:00<?, ?it/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5f3e83cdbb2406bba8f3de451ae5487",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Resolving data files:   0%|          | 0/39 [00:00<?, ?it/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16193d2661e945c59a998428e0008529",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Resolving data files:   0%|          | 0/39 [00:00<?, ?it/s]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Load phikon-v2\n",
        "processor = AutoImageProcessor.from_pretrained(\"owkin/phikon-v2\", use_fast=True)\n",
        "model = AutoModel.from_pretrained(\"owkin/phikon-v2\")\n",
        "model.eval()\n",
        "model.to(device)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09cf111444c845f188ed931789b9d450",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32ba1fc77ab34fd6880972bc99a3c5f6",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "preprocessor_config.json:   0%|          | 0.00/750 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86766c5c432a436b8a6124f58c468fe2",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "config.json: 0.00B [00:00, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62089c4fff9249908bd21b96aeb980d4",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "model.safetensors:   0%|          | 0.00/1.21G [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "Dinov2Model(\n  (embeddings): Dinov2Embeddings(\n    (patch_embeddings): Dinov2PatchEmbeddings(\n      (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (dropout): Dropout(p=0.0, inplace=False)\n  )\n  (encoder): Dinov2Encoder(\n    (layer): ModuleList(\n      (0-23): 24 x Dinov2Layer(\n        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attention): Dinov2Attention(\n          (attention): Dinov2SelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (output): Dinov2SelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (layer_scale1): Dinov2LayerScale()\n        (drop_path): Identity()\n        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): Dinov2MLP(\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (activation): GELUActivation()\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_scale2): Dinov2LayerScale()\n      )\n    )\n  )\n  (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "def load_and_process_image(sample):\n",
        "    image_bytes = sample[\"jpg\"]\n",
        "    # image = Image.open(io.BytesIO(image_bytes))\n",
        "    image = image_bytes\n",
        "    return {\n",
        "        \"image\": image,\n",
        "        \"label\": sample[\"json\"][\"label\"],\n",
        "        \"patient_id\": sample[\"__key__\"][:12] # TCGA-XX-XXXX\n",
        "    }\n",
        "\n",
        "ds_train_processed = ds_train.map(load_and_process_image, remove_columns=[\"jpg\", \"json\", \"__key__\", \"__url__\"])\n",
        "ds_test_processed = ds_test.map(load_and_process_image, remove_columns=[\"jpg\", \"json\", \"__key__\", \"__url__\"])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Exercise 1**: Visualize some of the H&E samples of the dataset and display corresponding the labels and patient ids.\n",
        "\n",
        "- Are histopathology images different from common images, e.g., as seen in the ImageNet-1k dataset?\n",
        "- If so, what implications would this have...\n",
        "    - to data distributions?\n",
        "    - to general pre-trained FMs for RGB images?\n",
        "    - to data pre-processing?\n",
        "    - ...\n",
        "\n",
        "\n",
        "Visualize some of the crops, print the label as well as as the patient id of the crop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "iter_ds = iter(ds_train_processed)\n",
        "\n",
        "### YOUR CODE to visualize images here ###\n",
        "# Hint: iterate over the data loader\n",
        "# use display from IPython.display to show images in a Jupyter notebook\n",
        "from IPython.display import display"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"task1.1\"></a>\n",
        "\n",
        "### Task 1.1: Linear Probing based on FM embeddings\n",
        "\n",
        "**Exercise 2**: \n",
        "1. Embed the training crops, store the patient ids as well as the labels. \n",
        "2. Visualize the embeddings using a UMAP\n",
        "3. Fit a Logistic Regression (LR) model on the training set\n",
        "4. Embed the testset crops, and evaluate the LR using accuracy and F1-score.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "ds_train_processed = ds_train_processed.with_format(\"torch\")\n",
        "ds_test_processed = ds_test_processed.with_format(\"torch\")\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EMBEDDING_SIZE = 1024\n",
        "NR_BATCHES = 100\n",
        "\n",
        "dl_train = torch.utils.data.DataLoader(\n",
        "    ds_train_processed,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=8,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "embeddings = torch.zeros((BATCH_SIZE * NR_BATCHES, EMBEDDING_SIZE))\n",
        "labels = []\n",
        "patient_ids = []\n",
        "\n",
        "with torch.inference_mode():\n",
        "    with torch.autocast(device.type, torch.bfloat16):\n",
        "        for i, batch in enumerate(tqdm(dl_train, total=NR_BATCHES)):\n",
        "            inputs = processor(batch[\"image\"], return_tensors=\"pt\").to(device)\n",
        "\n",
        "            ### YOUR CODE ###\n",
        "            # Hint: use model to get embeddings\n",
        "            # get the last hidden state and pool it to get a single embedding per image\n",
        "            ### END YOUR CODE ###\n",
        "\n",
        "\n",
        "            if i == (NR_BATCHES - 1):\n",
        "              break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Exercise 3**:\n",
        "\n",
        "Now that we have encoded our lung histopathology crops, visualize them in a UMAP.\n",
        "Using the same umap embedding, color the plot once by patient ids and once by tumor type.\n",
        "\n",
        "Create a second UMAP in which you visualize the embedded crops from only four patients, e.g. ['TCGA-33-4582', 'TCGA-55-8614', 'TCGA-77-8138', 'TCGA-71-8520'].\n",
        "\n",
        "- What do you see in the UMAPs?\n",
        "    - Are there overlapping classes?\n",
        "    - If so why, if not why not?\n",
        "    - How nicely are the cancer types clustering?\n",
        "    - Do we also see clustering in the patients? Why could this be?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Apply UMAP\n",
        "import seaborn as sns\n",
        "labels = np.array(labels)\n",
        "patient_ids = np.array(patient_ids)\n",
        "\n",
        "### YOUR CODE ###\n",
        "\n",
        "# Use below snippet from the UMAP library\n",
        "# UMAP(n_neighbors=30, min_dist=0.3, n_components=2, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we can do some logistic regression based on the FM embeddings to differentiate between Lung Adenocarcinoma (LUAD) and Lung Squamous Cell Carcinoma ( LUSC).\n",
        "\n",
        "First, train a Logistic Regression classifier, afterwards embed the first 100 batches with a batch size of 32 of the test dataset defined above and calculate the accuracy as well as the f1-score on it. Additionally, save the patient ids for the next task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "### YOUR CODE ###\n",
        "\n",
        "# Use a logistic regression model from sklearn to classify the embeddings\n",
        "# Train on the embeddings from the training set above and the labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "dl_test = torch.utils.data.DataLoader(\n",
        "    ds_test_processed,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=8,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "### YOUR CODE ###\n",
        "\n",
        "## now do the same what you did to calculate embeddings for train set, but for test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "### YOUR CODE ###\n",
        "\n",
        "# Use the trained logistic regression model to predict on the test embeddings\n",
        "# Calculate accuracy, F1-score and confusion matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"task1.2\"></a>\n",
        "\n",
        "\n",
        "### Task 1.2: Tissue-level classification\n",
        "\n",
        "Until now, we did everything on a crop level (256x256 pixels) extracted from a larger pathology whole slide that often spans more than 10,000 x 10,000 pixels.\n",
        "As we are interested in a tissue-level and not crop level task, it would make more sense to derive predictions on a patient level.\n",
        "\n",
        "We implement majority voting as a simple baseline to derive tissue-level prediction from the logisitic regression based test-set predictions\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\"pid\": patient_ids_test, \"prediction\": lr_predictions, \"label\": labels_test})\n",
        "df_majority_voting = df.groupby(\"pid\").agg({\"prediction\":pd.Series.mode, \"label\":\"first\"})\n",
        "\n",
        "predictions_tissue = df_majority_voting[\"prediction\"].values\n",
        "labels_tissue = df_majority_voting[\"label\"].values\n",
        "\n",
        "# resolve ties\n",
        "predictions_tissue = [p[0] if isinstance(p, np.ndarray) else p for p in predictions_tissue]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now calculate tissue-level accuracies and f1-score.\n",
        "- Would you expect improving performances? Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "### YOUR CODE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"task1.3\"></a>\n",
        "\n",
        "\n",
        "### Task 1.3: Whole-slide image analysis\n",
        "\n",
        "So far we have taken a look at tissue-level downstream tasks where we were interested in global labels. We can also take a look at more local labels in a whole slide image (WSI).\n",
        "\n",
        "In this task, we are using a WSI originating from a colorectal cancer (CRC) patient. A visualization of the WSI can be found [here](https://www.cycif.org/data/orion-crc/P37_S38-CRC10#s=0#w=0#g=5#m=-1#a=-100_-100#v=0.4618_0.8898_0.6185#o=-100_-100_1_1#p=Q).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import s3fs\n",
        "import os\n",
        "\n",
        "fs = s3fs.S3FileSystem(anon=True)\n",
        "s3_path = 'lin-2023-orion-crc/data/CRC10/18459_LSP10452_US_SCAN_OR_001__091355-registered.ome.tif'\n",
        "\n",
        "# Local filename to save the image\n",
        "local_filename = 'crc10_he_wsi.ome.tif'\n",
        "\n",
        "if not os.path.exists(local_filename):\n",
        "    # Download the file from S3 to local filesystem\n",
        "    fs.get(s3_path, local_filename)\n",
        "\n",
        "print(f'Downloaded {local_filename}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the image. To save some RAM and compute time, we only take a look at a 10,080 x 10,080 subpart of the WSI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import tifffile\n",
        "\n",
        "region = tifffile.imread(local_filename)\n",
        "region = region[15000:25080, 5000:15080]\n",
        "plt.imshow(region)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, embed the tissue by iterating over the image using a crop size of 224x224 without overlap, store the cls tokens and the local patch tokens of each of the crops.\n",
        "\n",
        "Afterwards, take the top left and the bottom right cls embedding as well as the mean of local patch embeddings of the top left as well as bottom right crop. These will serve as \"prototypes\". Cacluate the cosine similarities between all cls tokens and the cls-based prototypes and the patches and the patch-based prototypes, respectively.\n",
        "\n",
        "Hint: the patch size of the Phikon2 model is 16x16.\n",
        "\n",
        "**Exercise 4**:\n",
        "1. Implement the sliding window embedding \n",
        "2. Calculate cosine similarities of a prototype of the image and all other embeddings\n",
        "3. Visualize the cosine similarities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "embeddings_cls = torch.zeros((45,45, EMBEDDING_SIZE))\n",
        "embeddings_ps = torch.zeros((630,630, EMBEDDING_SIZE))\n",
        "\n",
        "region_x, region_y = region.shape[:2]\n",
        "\n",
        "with torch.inference_mode():\n",
        "    with torch.autocast(device.type, torch.bfloat16):\n",
        "        ### YOUR CODE ###\n",
        "        # inputs = processor(crop, return_tensors=\"pt\").to(device)\n",
        "        # outputs = model(**inputs)\n",
        "        # cls_tokens = outputs.last_hidden_state[:, 0, :].cpu()\n",
        "        # embeddings_cls[x,y] = cls_tokens\n",
        "        # patch_tokens = outputs.last_hidden_state[:, 1:, :].cpu()\n",
        "        # patch_tokens = patch_tokens.reshape((14,14, EMBEDDING_SIZE))\n",
        "        # embeddings_ps[x*14 : (x+1)*14, y*14 : (y+1) * 14] = patch_tokens\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Example Top left token\n",
        "# For the cls tokens first\n",
        "cls_00 = embeddings_cls[0, 0]  # shape: (EMBEDDING_SIZE,)\n",
        "cls_00 = cls_00.unsqueeze(0)  # shape: (1, EMBEDDING_SIZE)\n",
        "\n",
        "vectors_all_cls = embeddings_cls.view(-1, embeddings_cls.shape[2])\n",
        "cosine_sim_cls_00 = F.cosine_similarity(cls_00, vectors_all_cls, dim=1)\n",
        "cosine_sim_cls_00 = cosine_sim_cls_00.reshape(embeddings_cls.shape[:2])\n",
        "\n",
        "# For the ps tokens \n",
        "ps_00 = embeddings_ps[0, 0]  # shape: (EMBEDDING_SIZE,)\n",
        "ps_00 = torch.mean(embeddings_ps[:14, :14], dim=(0,1))\n",
        "ps_00 = ps_00.unsqueeze(0)  # shape: (1, EMBEDDING_SIZE)\n",
        "\n",
        "vectors_all_ps = embeddings_ps.view(-1, embeddings_ps.shape[2])\n",
        "cosine_sim_ps_00 = F.cosine_similarity(ps_00, vectors_all_ps, dim=1)\n",
        "cosine_sim_ps_00 = cosine_sim_ps_00.reshape(embeddings_ps.shape[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the cosine similarities on top of the image between the selected prototypes and all other embeddings (once for the cls tokens, once for the patch tokens).\n",
        "\n",
        "- What do you observe?\n",
        "- What could this be useful for in a clinical setting?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "from skimage.transform import resize\n",
        "\n",
        "\n",
        "\n",
        "cosine_sim_cls_00_resized = resize(np.array(cosine_sim_cls_00), (region_x, region_y))\n",
        "cosine_sim_cls_nn_resized = resize(np.array(cosine_sim_cls_nn), (region_x, region_y))\n",
        "\n",
        "cosine_sim_ps_00_resized = resize(np.array(cosine_sim_ps_00), (region_x, region_y))\n",
        "cosine_sim_ps_nn_resized = resize(np.array(cosine_sim_ps_nn), (region_x, region_y))\n",
        "\n",
        "\n",
        "plt.imshow(region[::10, ::10])\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "fig, axs = plt.subplots(2,3, figsize=(12, 5))\n",
        "axs = axs.flatten()\n",
        "axs[0].imshow(region[:224, :224])\n",
        "axs[0].set_title(\"Top left crop (224x224)\")\n",
        "axs[1].imshow(region[::10, ::10])\n",
        "axs[1].imshow(cosine_sim_cls_00_resized[::10, ::10], alpha=0.3, cmap=\"jet\")\n",
        "axs[2].imshow(region[::10, ::10])\n",
        "axs[2].imshow(cosine_sim_ps_00_resized[::10, ::10], alpha=0.3, cmap=\"jet\")\n",
        "axs[3].imshow(region[-224:, -224:])\n",
        "axs[3].set_title(\"Bottom right crop (224x224)\")\n",
        "axs[4].imshow(region[::10, ::10])\n",
        "axs[4].imshow(cosine_sim_cls_nn_resized[::10, ::10], alpha=0.3, cmap=\"jet\")\n",
        "axs[5].imshow(region[::10, ::10])\n",
        "axs[5].imshow(cosine_sim_ps_nn_resized[::10, ::10], alpha=0.3, cmap=\"jet\")\n",
        "\n",
        "axs[1].set_title(\"Cos. sim (CLS)\")\n",
        "axs[2].set_title(\"Cos. sim (Patch)\")\n",
        "\n",
        "for ax in axs:\n",
        "    ax.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}